{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is to check images by labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import itertools\n",
    "from sklearn.utils import shuffle\n",
    "import os\n",
    "import json\n",
    "from keras import Sequential\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import backend as K\n",
    "from keras.layers import Conv2D, MaxPool2D, Dropout, Dense, Flatten\n",
    "%matplotlib inline \n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utility method to read the data pickle\n",
    "def read_dataset(filename):\n",
    "    return pd.read_pickle(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_PICKLE = '../data/pickles/test.pickle'\n",
    "TRAIN_PICKLE = '../data/pickles/train.pickle'\n",
    "VALIDATION_PICKLE = '../data/pickles/validation.pickle'\n",
    "TRAIN_LABEL_PICKLE = '../data/pickles/train_label.pickle'\n",
    "VALIDATION_LABEL_PICKLE = '../data/pickles/validation_label.pickle'\n",
    "PATH_TO_IMAGES = 'G:\\\\Data\\\\data\\\\train\\\\'\n",
    "\n",
    "LABEL_ID = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = read_dataset(VALIDATION_LABEL_PICKLE)\n",
    "dataset = pd.DataFrame(dataset, dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_labels = dataset['labelId'].nunique() # Number of distinct labels\n",
    "maximum_label_id = max(dataset['labelId']) # The maximum labelId value\n",
    "number_of_images = dataset['imageId'].nunique() # Number of distinct images\n",
    "print('Number of distinct labels in the dataset : ', number_of_labels)\n",
    "print('Maximum id if labels in the dataset : ', maximum_label_id)\n",
    "print('Number of distinct images in the dataset : ', number_of_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now let us a define a function for make undersample data with different proportion\n",
    "#different proportion means with different proportion of normal classes of data\n",
    "def undersample(unlabelled_data,labelled_data, times):#times denote the normal data = times*fraud data\n",
    "    count_labelled = len(labelled_data)\n",
    "    healthy_sample = unlabelled_data.sample(times*count_labelled, replace=False)\n",
    "    healthy_sample.describe()\n",
    "    total_count = len(healthy_sample) + count_labelled\n",
    "    print(\"Number of unlabelled rows :\",len(healthy_sample))\n",
    "    print(\"Number of labelled rows :\",count_labelled)\n",
    "    print(\"total number of record in resampled data is:\",total_count)\n",
    "    return healthy_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for undersampling we need a portion of majority class and will take whole data of minority class\n",
    "# count fraud transaction is the total number of fraud transaction\n",
    "# now lets us see the index of fraud cases\n",
    "labelled_data= dataset[dataset['labelId'] == LABEL_ID]\n",
    "labelled_data.reset_index()\n",
    "unlabelled_data= dataset[(dataset['labelId'] != LABEL_ID )\n",
    "                         &( ~dataset['imageId'].isin(labelled_data['imageId']))]\n",
    "unlabelled_data.reset_index()\n",
    "undersampled_data = undersample(unlabelled_data,labelled_data, 1)\n",
    "train_images = pd.concat([labelled_data, undersampled_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images['isLabel'] = train_images['labelId'] == LABEL_ID\n",
    "train_images = train_images[['imageId', 'isLabel']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images.head()\n",
    "shuffle(train_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(y_true, y_pred):\n",
    "    # Count positive samples.\n",
    "    c1 = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    c2 = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    c3 = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    # If there are no true samples, fix the F1 score at 0.\n",
    "    if c3 == 0:\n",
    "        return 0\n",
    "    # How many selected items are relevant?\n",
    "    precision = c1 / c2\n",
    "    # How many relevant items are selected?\n",
    "    recall = c1 / c3\n",
    "    # Calculate f1_score\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    return f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(input_shape):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(filters=32, kernel_size = (5,5),padding = 'Same', activation ='relu', input_shape =input_shape))\n",
    "    model.add(Conv2D(filters=32, kernel_size = (5,5), padding = 'Same', activation = 'relu'))\n",
    "    model.add(MaxPool2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Conv2D(filters=64, kernel_size = (3,3), padding = 'Same', activation = 'relu'))\n",
    "    model.add(Conv2D(filters=64, kernel_size = (3,3), padding = 'Same', activation = 'relu'))\n",
    "    model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(units=256, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(units=1, activation='softmax'))\n",
    "\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model((28,28,1))\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = [f1_score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the augmentation configuration we will use for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
